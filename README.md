# HQâ€‘SAMâ€‘Gate: IoUâ€‘Gated Mask Refinement for DMAOT (VOTS 2024 Runnerâ€‘up)

> **DMAOT + HQâ€‘SAM with IoUâ€‘gated refinement** for robust multiâ€‘object tracking and highâ€‘quality masks. This repo contains the exact runners we used for our **2ndâ€‘place** VOTSÂ 2024 entry, plus ablations (backbones, thresholds, parallelism, visualization, depth, and cycle priors).

<p align="center">
  <img src="hqsam_gate_diagram.png" alt="HQ-SAM-Gate method diagram" width="800">
  <br>
  <a href="hqsam_gate_diagram.pdf">Download PDF</a> Â·
  <a href="docs/diagram.html">View PDF inline (GitHub Pages)</a>
</p>


---

<p align="center">
  <img alt="Status" src="https://img.shields.io/badge/status-active-brightgreen"> 
  <img alt="Python" src="https://img.shields.io/badge/python-3.9%20|%203.10%20|%203.11-blue"> 
  <img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-%E2%89%A5%202.1-red"> 
  <img alt="CUDA" src="https://img.shields.io/badge/CUDA-11.8%20|%2012.x-orange"> 
  <img alt="License" src="https://img.shields.io/badge/license-TBD-lightgrey">
</p>

## âœ¨ Whatâ€™s inside
- **HQâ€‘SAMâ€‘Gate pipeline**: **DMAOT** predicts coarse perâ€‘object masks â†’ **HQâ€‘SAM** refines them â†’ **IoUâ€‘based rejection** filters spurious proposals â†’ (optional) **cycle prior** feeds highâ€‘confidence HQâ€‘SAM masks back to DMAOT for the next frame.
- **Strong results**: +1.3% **Q** on VOTS test over DMAOT baseline **without any extra training**.
- **Lowâ€‘VRAM friendly**: compared to vanilla AOT (which can spike to ~80â€¯GB on long videos), HQâ€‘SAMâ€‘Gate + our runners are engineered for stability. Use the `vparallel` scripts for very long sequences.
- **Tracker zoo**: scripts for **AOT/DeAOT** with **ResNetâ€‘50, Swinâ€‘S, Swinâ€‘B**, with or without **Domainâ€‘Memory (dm)**, with **HQâ€‘SAM** integration, depth augmentation, visualization, and SLURM.

---

## ğŸ—‚ï¸ Script naming & variants
Most entrypoints are explicit **oneâ€‘fileâ€‘perâ€‘variant** for clarity.

```
python_[<backbone>_][dm_]<aot|deaot|aotl|aotb|aots|aott|r50|swinb>
       [_integrate_SAM[_hq]][_h__<NN>][_<extra>].py
```

**Legend**

| Token | Meaning |
|---|---|
| `aot`, `deaot` | AOT family or DeAOT core. |
| `aott`, `aots`, `aotb`, `aotl` | AOTâ€‘T/S/B/L variants. |
| `r50`, `swinb` | ResNetâ€‘50 AOTL or Swinâ€‘B backbone. |
| `dm_` prefix | **Domainâ€‘Memory** channel enabled. |
| `integrate_SAM` | Uses **SAM / SAMâ€‘HQ** for refinement. |
| `hq` | Highâ€‘quality **HQâ€‘SAM** mode. |
| `h__NN` | IoU **rejection threshold Ï„** â‰ˆ `NN/100` (e.g., `h__59` â†’ Ï„â‰ˆ0.59). |
| `depth` | Depthâ€‘aware variant. |
| `cycle` | Feeds HQâ€‘SAM highâ€‘confidence mask as a prior to DMAOT for t+1. |
| `vots`, `vtest` | VOTS/VOT evaluation wrappers. |
| `visualize*` | Dump overlays, embeddings, or videos. |
| `vparallel_*` | Multiâ€‘GPU parallel flavors tuned for A100/V100. |

**Good starting points**

- `python_swinb_dm_deaot_integrate_SAM_hq_h__59.py` â€“ strong singleâ€‘GPU runner (Swinâ€‘B, DeAOT, Domainâ€‘Memory, HQâ€‘SAM, Ï„â‰ˆ0.59).
- `python_swinb_dm_deaot_integrate_SAM_hq_h__59_vparallel_v3_A100.py` â€“ multiâ€‘GPU longâ€‘video runner.
- `python_swinb_dm_deaot_vots.py` â€“ VOTS evaluator wrapper for submission artifacts.

---

## ğŸ§  How HQâ€‘SAMâ€‘Gate works (overview)
1) **DMAOT** performs objectâ€‘wise propagation with longâ€‘term memories, producing **coarse masks** for all tracked objects in frame *t*.
2) **HQâ€‘SAM** receives each coarse mask as a **visual prompt** and returns **multiple mask proposals** (typically 3) per object.
3) **IoUâ€‘gated selection** chooses the proposal with the highest IoU vs. DMAOTâ€™s mask **only if** `max IoU > Ï„`; otherwise the DMAOT mask is kept.
4) *(Optional)* **Cycle prior**: highâ€‘confidence HQâ€‘SAM masks are passed back to DMAOT to guide predictions at *t+1*.

**Selection rule** (per object i):

```
Let S = {s1, s2, s3} be HQâ€‘SAM proposals.
If max_{sâˆˆS} IoU(s, o_t,i) > Ï„ : o_t,i â† argmax_{sâˆˆS} IoU(s, o_t,i)
Else:                                   keep o_t,i
```

**Quality metric Q** over a dataset of N sequences (T_s frames, N_s objects):

```
Q = (1/N) * Î£_s [ (1/(T_s*N_s)) * Î£_t Î£_i IoU(o_{s,t,i}, g_{s,t,i}) ]
```

---

## ğŸ“Š Results (VOTSÂ 2023/2024)
> Dev/test numbers reproduced from our report. Replace/add your latest if youâ€™ve reâ€‘run.  

**Backbone comparison (DMAOT dev split)**

| Backbone | #Params | Q (dev) |
|---|---:|---:|
| ResNetâ€‘50 | 23M | 0.6386 |
| Swinâ€‘S | 50M | 0.6418 |
| **Swinâ€‘B** | **88M** | **0.6817** |

**Mask refinement ablations (on top of DMAOT, Swinâ€‘B)**

| Method | Q (dev) | Q (test) | Notes |
|---|---:|---:|---|
| No maskâ€‘refinement | 0.6817 | 0.6400 | DMAOT baseline |
| SAMâ€‘B | 0.6971 | â€“ | Light SAM refinement |
| SAMâ€‘L | 0.7298 | 0.6187 | Larger model, no gating |
| + Rejection Sampling | 0.7326 | 0.6458 | **Ï„â€‘gated** selection |
| HQâ€‘SAMâ€‘L | â€“ | â€“ |  |
| + Rejection Sampling | 0.7298 | 0.6513 | Highâ€‘quality token |
| + Cycle | 0.7046 | 0.6439 | Prior fed back to DMAOT |
| **HQâ€‘SAMâ€‘H + Rejection** | **0.7335** | **0.6530** | **Best: +1.3% over baseline (test)** |

> Tip: tune `Ï„` via the `h__NN` suffix. We found **0.55â€“0.65** a good search range; `h__59` worked well for VOTSÂ 2024.

---

## ğŸš€ Quickstart

### 1) Environment
```bash
conda create -n hqsam_gate python=3.10 -y
conda activate hqsam_gate
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install numpy scipy opencv-python pillow tqdm einops yacs pyyaml timm decord matplotlib
# SAM / SAM-HQ (install per their repos) and place checkpoints accordingly
```

### 2) Data (VOTS)
```
DATASETS/
  VOTS2024/
    sequences/
      <seq_name>/
        imgs/000000.jpg ...
        init.json  # VOTS prompts
```

### 3) Checkpoints
```
checkpoints/
  aot/   deaot/   sam/   sam_hq/
```
Adjust paths via CLI (`--ckpt-*`) in each runner.

### 4) Singleâ€‘GPU example
```bash
python python_swinb_dm_deaot_integrate_SAM_hq_h__59.py \
  --data-root /path/to/DATASETS/VOTS2024 \
  --seq carchase \
  --output runs/vots24_swinb_deaot_dm_hq59 \
  --save-vis
```

### 5) Multiâ€‘GPU parallel (long videos)
```bash
python python_swinb_dm_deaot_integrate_SAM_hq_h__59_vparallel_v3_A100.py \
  --data-root /path/to/DATASETS/VOTS2024 \
  --list lists/vots24_test.txt \
  --gpus 0,1,2,3 \
  --output runs/vots24_parallel
```

### 6) VOTS submission pack
```bash
python python_swinb_dm_deaot_vots.py \
  --data-root /path/to/DATASETS/VOTS2024 \
  --list lists/vots24_test.txt \
  --output runs/vots24_submit
# Zip the produced folder as per VOTS instructions
```

### 7) SLURM example
```bash
sbatch -J hqsam_gate_vots -p a100_80gb --gres=gpu:4 --cpus-per-task=16 --mem=64G \
  --wrap "python python_swinb_dm_deaot_integrate_SAM_hq_h__59_vparallel_v3_A100.py \
           --data-root /scratch/DATASETS/VOTS2024 \
           --list lists/vots24_test.txt \
           --output runs/vots24_parallel_a100"
```

---

## ğŸ”§ Tips & gotchas
- **VRAM budgeting**: for long sequences, prefer `vparallel` variants; reduce objects per pass; lower input resolution if needed.
- **Thin structures / fine detail**: keep HQâ€‘SAM enabled; raise Ï„ a bit to avoid blurry proposals.
- **Speed**: `*_vfast.py` favors throughput; AOTâ€‘T memoryâ€‘optimized runner is also provided.
- **Debug**: `*_visualize*.py` scripts export embeddings, masks, and video previews.

---

## ğŸ“š References & citation
Please cite the respective AOT/DeAOT, SAM/SAMâ€‘HQ papers, and this repository.

```bibtex
@inproceedings{HQSAM_Gate_VOTS2024,
  title={HQ-SAM-Gate: IoU-Gated Mask Refinement for DMAOT},
  author={Soltani Kazemi, Elham and Toubal, Imad Eddine and Rahmon, Gani and Collins, Jaired and Mogollon, Juan and Hatuwal, Bijaya and Palaniappan, K.},
  booktitle={VOTS Challenge},
  year={2024},
  note={2nd place}
}
```

---

## ğŸ‘©â€ğŸ’» Maintainer
**Elham Soltani Kazemi**  
PhD Candidate, Computer Science (Vision & AI) â€” University of Missouri  
Issues and PRs welcome (new backbones, memory policies, or deployment scripts).


## ğŸ–¼ï¸ Qualitative Results (VOTS)
> Selected frames illustrating DMAOT â†’ HQâ€‘SAM â†’ IoUâ€‘gated refinement; see captions in the PDF for details.

<p align="center"><img src="qualitative_page_01.png" alt="qualitative 1" width="49%"></p>

